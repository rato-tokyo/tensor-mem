# Infini-Attention è«–æ–‡ã¾ã¨ã‚ã¨å®Ÿè£…æ¯”è¼ƒ

**è«–æ–‡**: "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"
**è‘—è€…**: Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal (Google)
**arXiv**: 2404.07143v2

---

## 1. è«–æ–‡ã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢

### 1.1 å•é¡Œè¨­å®š

æ¨™æº–çš„ãªTransformerã®Attentionã¯ï¼š
- **ãƒ¡ãƒ¢ãƒªè¨ˆç®—é‡**: O(NÂ²) - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®2ä¹—ã«æ¯”ä¾‹
- **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«æ¯”ä¾‹ã—ã¦å¢—å¤§
- ä¾‹: 500Bãƒ¢ãƒ‡ãƒ«ã€ãƒãƒƒãƒ512ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ2048ã§ **3TB** ã®ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆ

### 1.2 è§£æ±ºç­–: Infini-attention

**åœ§ç¸®ãƒ¡ãƒ¢ãƒªï¼ˆCompressive Memoryï¼‰** ã‚’å°å…¥ã—ã€å›ºå®šã‚µã‚¤ã‚ºã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’ä¿æŒï¼š

```
Os, Ms = infini-attention(Xs, Ms-1)
```

- `Xs`: ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå…¥åŠ›
- `Ms-1`: å‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹
- `Os`: å‡ºåŠ›
- `Ms`: æ›´æ–°ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªçŠ¶æ…‹

---

## 2. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

### 2.1 ãƒ¡ãƒ¢ãƒªæ§‹é€ 

**é€£æƒ³è¡Œåˆ—ï¼ˆAssociative Matrixï¼‰** ã‚’ä½¿ç”¨ï¼š

| è¦ç´  | å½¢çŠ¶ | èª¬æ˜ |
|------|------|------|
| M | `[d_key, d_value]` | KVãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ ¼ç´ |
| z | `[d_key]` | æ­£è¦åŒ–é …ï¼ˆã‚­ãƒ¼ã®ç´¯ç©å’Œï¼‰ |

**ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆ**: `d_key Ã— d_value + d_key` per head per layer
ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«**ä¾å­˜ã—ãªã„**å›ºå®šã‚µã‚¤ã‚ºï¼‰

### 2.2 ãƒ¡ãƒ¢ãƒªæ¤œç´¢ï¼ˆMemory Retrievalï¼‰

```
A_mem = Ïƒ(Q) @ M_{s-1} / (Ïƒ(Q) @ z_{s-1})
```

- `Ïƒ`: æ´»æ€§åŒ–é–¢æ•°ï¼ˆ**ELU + 1** ã‚’ä½¿ç”¨ï¼‰
- `Q`: ã‚¯ã‚¨ãƒªï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ï¼‰
- è«–æ–‡ã§ã¯ `Ïƒ(Q)` ã‚’ä½¿ç”¨ã™ã‚‹ãŒã€å½“å®Ÿè£…ã§ã¯ raw Q ã‚’ä½¿ç”¨

### 2.3 ãƒ¡ãƒ¢ãƒªæ›´æ–°ï¼ˆMemory Updateï¼‰

**Linearæ›´æ–°**:
```
Ms = M_{s-1} + Ïƒ(K)^T @ V
zs = z_{s-1} + Î£ Ïƒ(K_t)
```

**Deltaæ›´æ–°**ï¼ˆæ”¹è‰¯ç‰ˆï¼‰:
```
Ms = M_{s-1} + Ïƒ(K)^T @ (V - Ïƒ(K) @ M_{s-1} / (Ïƒ(K) @ z_{s-1}))
```

Deltaæ›´æ–°ã¯ã€æ—¢å­˜ã®KVãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒã‚ã‚‹å ´åˆã¯ãƒ¡ãƒ¢ãƒªã‚’å¤‰æ›´ã—ãªã„ã€‚

### 2.4 ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹

ãƒ­ãƒ¼ã‚«ãƒ«Attentionï¼ˆ`A_dot`ï¼‰ã¨ãƒ¡ãƒ¢ãƒªæ¤œç´¢ï¼ˆ`A_mem`ï¼‰ã‚’å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆã§çµåˆï¼š

```
A = sigmoid(Î²) âŠ™ A_mem + (1 - sigmoid(Î²)) âŠ™ A_dot
```

- `Î²`: å­¦ç¿’å¯èƒ½ãªã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆãƒ˜ãƒƒãƒ‰ã”ã¨ï¼‰
- è«–æ–‡å®Ÿé¨“ã§ã¯ã€å­¦ç¿’å¾Œã«ãƒ˜ãƒƒãƒ‰ãŒ **ç‰¹åŒ–å‹**ï¼ˆÎ²â‰ˆ0 or Î²â‰ˆ1ï¼‰ã¨ **æ··åˆå‹**ï¼ˆÎ²â‰ˆ0.5ï¼‰ã«åˆ†åŒ–

### 2.5 ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

**é‡è¦**: è«–æ–‡ Section 4.1 (Position Embeddings)

> "we don't use position embeddings for the key and query vectors of the compressive memory to store only global contextual information in the long-term memory. The PEs were applied to the QK vectors only after the compressive memory reading and update."

- **ãƒ­ãƒ¼ã‚«ãƒ«Attention**: RoPEï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Šï¼‰
- **ãƒ¡ãƒ¢ãƒª**: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**ãªã—**ï¼ˆNoPEï¼‰

---

## 3. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã¨BPTT

### 3.1 ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°

è«–æ–‡ Section 4.1:
> "we forward-pass the entire input text a Transformer model and then perform segment chunking at each Infini-attention layer"

- å…¥åŠ›å…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™
- å„Infini-attentionå±¤ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã‚’å®Ÿè¡Œ
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã«å‡¦ç†ã—ã€çµåˆã—ã¦æ¬¡ã®å±¤ã¸

### 3.2 BPTTï¼ˆBackpropagation Through Timeï¼‰

> "Each Infini-attention layer is trained with back-propagation through time (BPTT) by computing the gradient w.r.t the compressive memory states"

- ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’é€šã˜ãŸå‹¾é…è¨ˆç®—
- ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚gradient checkpointingä½¿ç”¨

### 3.3 å‡¦ç†é †åºï¼ˆé‡è¦ï¼‰

è«–æ–‡ã®å›³ã¨å¼ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹é †åºï¼š

```
Segment s:
1. M_{s-1} ã‹ã‚‰ãƒ¡ãƒ¢ãƒªæ¤œç´¢ï¼ˆretrieveï¼‰
2. ãƒ­ãƒ¼ã‚«ãƒ«Attentionè¨ˆç®—
3. ã‚²ãƒ¼ãƒˆã§çµåˆ
4. M_{s-1} ã‚’ M_s ã«æ›´æ–°ï¼ˆupdateï¼‰
5. M_s ã‚’æ¬¡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ s+1 ã¸æ¸¡ã™
```

**æ³¨æ„**: retrieve â†’ update ã®é †åºï¼ˆå› æœæ€§ç¶­æŒï¼‰

---

## 4. å®Ÿé¨“çµæœ

### 4.1 è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆPG19, Arxiv-mathï¼‰

| ãƒ¢ãƒ‡ãƒ« | ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º | åœ§ç¸®ç‡ | PG19 PPL | Arxiv-math PPL |
|--------|------------|--------|----------|----------------|
| Transformer-XL | 50M | 3.7x | 11.88 | 2.42 |
| Memorizing Transformers | 183M | 1x | 11.37 | 2.26 |
| RMT | 2.5M | 73x | 13.27 | 2.55 |
| **Infini-Transformer (Linear)** | 1.6M | **114x** | **9.65** | 2.24 |
| **Infini-Transformer (Linear + Delta)** | 1.6M | **114x** | 9.67 | **2.23** |

- è¨­å®š: 12å±¤ã€8ãƒ˜ãƒƒãƒ‰ã€d=128ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·N=2048ã€å…¥åŠ›é•·32768
- 100Ké•·å­¦ç¿’ã§Arxiv-mathãŒ2.20ã¾ã§æ”¹å–„

### 4.2 Passkeyæ¤œç´¢ï¼ˆ1Mã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰

| ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•· | 32K | 128K | 256K | 512K | 1M |
|--------------|-----|------|------|------|-----|
| Zero-shot (Linear) | 14/13/98 | 11/14/100 | 6/3/100 | 6/7/99 | 8/6/98 |
| Fine-tuned (Linear) | 100/100/100 | 100/100/100 | 100/100/100 | 97/99/100 | 96/94/100 |
| Fine-tuned (Linear + Delta) | 100/100/100 | 100/100/99 | 100/100/99 | 100/100/100 | **100/100/100** |

- 1Bãƒ¢ãƒ‡ãƒ«ã€**5Ké•·**ã§å­¦ç¿’ â†’ **1Mé•·**ã§è©•ä¾¡
- Fine-tuning: 400 steps

### 4.3 æ›¸ç±è¦ç´„ï¼ˆBookSum, 500Kï¼‰

| ãƒ¢ãƒ‡ãƒ« | Rouge-1 | Rouge-2 | Rouge-L | Overall |
|--------|---------|---------|---------|---------|
| PRIMERA + Unlimiformer | 37.9 | 8.2 | 16.3 | 17.2 |
| **Infini-Transformers (Linear + Delta)** | **40.0** | **8.8** | **17.9** | **18.5** |

- 8Bãƒ¢ãƒ‡ãƒ«ã€8Ké•·ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ â†’ 32Ké•·ã§fine-tune â†’ 500Ké•·ã§è©•ä¾¡

---

## 5. ç¾åœ¨ã®Senriå®Ÿè£…ã¨ã®æ¯”è¼ƒ

### 5.1 ä¸€è‡´ã—ã¦ã„ã‚‹ç‚¹

| é …ç›® | è«–æ–‡ | Senriå®Ÿè£… | çŠ¶æ…‹ |
|------|------|----------|------|
| ãƒ¡ãƒ¢ãƒªæ§‹é€  | é€£æƒ³è¡Œåˆ— M, z | TensorMemory(M, z) | âœ… ä¸€è‡´ |
| ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º | `[d_key, d_value]` per head | `[heads, head_dim, head_dim]` | âœ… ä¸€è‡´ |
| æ­£è¦åŒ–é … | z = Î£ k | z = keys.sum(dim=seq) | âœ… ä¸€è‡´ |
| ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° | ãƒ­ãƒ¼ã‚«ãƒ«:RoPE, ãƒ¡ãƒ¢ãƒª:NoPE | ãƒ­ãƒ¼ã‚«ãƒ«:RoPE, ãƒ¡ãƒ¢ãƒª:NoPE | âœ… ä¸€è‡´ |
| ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ | sigmoid(Î²) per head | sigmoid(memory_gate) per head | âœ… ä¸€è‡´ |
| ã‚²ãƒ¼ãƒˆåˆæœŸå€¤ | è«–æ–‡ã§ã¯å­¦ç¿’å¾Œã«åˆ†åŒ– | 0.0ã§åˆæœŸåŒ– | âœ… å¦¥å½“ |

### 5.2 ç›¸é•ç‚¹ï¼ˆè¦ä¿®æ­£ï¼‰

| é …ç›® | è«–æ–‡ | Senriå®Ÿè£… | é‡è¦åº¦ | ä¿®æ­£æ¡ˆ |
|------|------|----------|--------|--------|
| **æ´»æ€§åŒ–é–¢æ•°Ïƒ** | ELU + 1 | ãªã—ï¼ˆraw K, Qï¼‰ | âš ï¸ ä¸­ | Ïƒ(K), Ïƒ(Q)ã‚’è¿½åŠ  |
| **æ›´æ–°é †åº** | retrieve â†’ update | update â†’ retrieve | ğŸ”´ é«˜ | ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã§è§£æ±ºå¯èƒ½ |
| **Deltaæ›´æ–°** | å®Ÿè£…ã‚ã‚Š | æœªå®Ÿè£… | âš ï¸ ä¸­ | ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ  |
| **ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†** | ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã«å‡¦ç† | å…¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸€æ‹¬ | ğŸ”´ é«˜ | ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚’å®Ÿè£… |

### 5.3 è©³ç´°åˆ†æ

#### 5.3.1 æ´»æ€§åŒ–é–¢æ•°Ïƒã®æ¬ å¦‚

è«–æ–‡ã§ã¯ `Ïƒ(K) = ELU(K) + 1` ã‚’ä½¿ç”¨ï¼š
- è² ã®å€¤ã‚’é˜²ãã€æ­£è¦åŒ–ã®å®‰å®šæ€§ã‚’å‘ä¸Š
- Linear attention ã®æ¨™æº–çš„ãªæ‰‹æ³•

ç¾åœ¨ã®å®Ÿè£…:
```python
# base_memory.py
delta_M = torch.einsum("bhsd,bhse->bhde", values, keys)  # raw keys
```

ä¿®æ­£æ¡ˆ:
```python
def elu_plus_one(x):
    return F.elu(x) + 1

sigma_k = elu_plus_one(keys)
sigma_q = elu_plus_one(queries)
delta_M = torch.einsum("bhsd,bhse->bhde", values, sigma_k)
```

#### 5.3.2 æ›´æ–°é †åºï¼ˆupdate â†’ retrieve vs retrieve â†’ updateï¼‰

**è«–æ–‡ã®é †åº**: retrieve â†’ update
- å› æœæ€§ç¶­æŒï¼šéå»ã®ãƒ¡ãƒ¢ãƒªã‹ã‚‰æ¤œç´¢ã—ã¦ã‹ã‚‰ã€ç¾åœ¨ã®KVã§æ›´æ–°
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ`s`ã®å‡¦ç†ã§`M_{s-1}`ã‹ã‚‰æ¤œç´¢ã—ã€`M_s`ã«æ›´æ–°

**ç¾åœ¨ã®å®Ÿè£…**: update â†’ retrieve
- å˜ä¸€forward-passå­¦ç¿’ã§ã¯å¿…è¦ï¼ˆãƒ¡ãƒ¢ãƒªãŒç©ºã®çŠ¶æ…‹ã§retrieveã—ã¦ã‚‚æ„å‘³ãŒãªã„ï¼‰
- å³å¯†ãªå› æœæ€§ã¯å¤±ã‚ã‚Œã‚‹ãŒã€ç¾å®Ÿçš„ãªå¦¥å”

**æ ¹æœ¬çš„ãªè§£æ±ºç­–**: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå˜ä½ã®å‡¦ç†ã‚’å®Ÿè£…
```python
# é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²
for segment in chunks(sequence, segment_length):
    # 1. éå»ã®ãƒ¡ãƒ¢ãƒªã‹ã‚‰æ¤œç´¢
    mem_output = memory.retrieve(segment_queries)
    # 2. ãƒ­ãƒ¼ã‚«ãƒ«Attention
    local_output = local_attention(segment)
    # 3. çµåˆ
    output = gate * mem_output + (1-gate) * local_output
    # 4. ãƒ¡ãƒ¢ãƒªæ›´æ–°
    memory.update(segment_keys, segment_values)
```

#### 5.3.3 Deltaæ›´æ–°ã®æœªå®Ÿè£…

Deltaæ›´æ–°ã¯æ—¢å­˜ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®é‡è¤‡ã‚’é¿ã‘ã‚‹æ”¹è‰¯ç‰ˆï¼š
```python
# Linearæ›´æ–°
Ms = Ms_prev + sigma_K.T @ V

# Deltaæ›´æ–°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
retrieved = sigma_K @ Ms_prev / (sigma_K @ zs_prev)
Ms = Ms_prev + sigma_K.T @ (V - retrieved)
```

è«–æ–‡å®Ÿé¨“ã§ã¯Deltaæ›´æ–°ãŒè‹¥å¹²è‰¯ã„çµæœã‚’ç¤ºã—ã¦ã„ã‚‹ï¼ˆç‰¹ã«BookSumï¼‰ã€‚

---

## 6. å®Ÿè£…å„ªå…ˆåº¦

### é«˜å„ªå…ˆåº¦ï¼ˆå‹•ä½œã«å½±éŸ¿ï¼‰

1. **ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®å®Ÿè£…**
   - é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
   - retrieve â†’ update ã®é †åºã‚’ç¶­æŒ
   - BPTTå¯¾å¿œ

### ä¸­å„ªå…ˆåº¦ï¼ˆæ€§èƒ½æ”¹å–„ï¼‰

2. **æ´»æ€§åŒ–é–¢æ•°Ïƒã®è¿½åŠ **
   - ELU + 1 ã‚’ K, Q ã«é©ç”¨
   - æ•°å€¤å®‰å®šæ€§ã®å‘ä¸Š

3. **Deltaæ›´æ–°ã®å®Ÿè£…**
   - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
   - é•·æ–‡ã§ã®æ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã§ãã‚‹

### ä½å„ªå…ˆåº¦ï¼ˆæœ€é©åŒ–ï¼‰

4. **ã‚²ãƒ¼ãƒˆåˆæœŸåŒ–ã®èª¿æ•´**
   - è«–æ–‡ã§ã¯å­¦ç¿’å¾Œã«è‡ªç„¶ã«åˆ†åŒ–
   - ç¾åœ¨ã®0.0åˆæœŸåŒ–ã§å•é¡Œãªã—

---

## 7. å­¦ç¿’è¨­å®šï¼ˆè«–æ–‡ã‹ã‚‰ï¼‰

| é …ç›® | å€¤ |
|------|-----|
| ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ | Adafactor |
| å­¦ç¿’ç‡ï¼ˆLMï¼‰ | 0.01 |
| å­¦ç¿’ç‡ï¼ˆLLMç¶™ç¶šå­¦ç¿’ï¼‰ | 0.0001 |
| ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— | 1000 steps, linear |
| ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© | cosine decay |
| ãƒãƒƒãƒã‚µã‚¤ã‚º | 64 |
| ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•· | 2048 |
| å…¥åŠ›é•·ï¼ˆå­¦ç¿’ï¼‰ | 32768 |

---

## 8. ä»–ãƒ¢ãƒ‡ãƒ«ã¨ã®ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆæ¯”è¼ƒï¼ˆTable 1ï¼‰

| ãƒ¢ãƒ‡ãƒ« | ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆ | æœ‰åŠ¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•· | ãƒ¡ãƒ¢ãƒªæ›´æ–° | ãƒ¡ãƒ¢ãƒªæ¤œç´¢ |
|--------|---------------------|-------------------|-----------|-----------|
| Transformer-XL | `(d_key + d_value) Ã— H Ã— N Ã— l` | `N Ã— l` | ç ´æ£„ | Dot-product attention |
| Compressive Transformer | `d_model Ã— (c + N) Ã— l` | `(c Ã— r + N) Ã— l` | ç ´æ£„ | Dot-product attention |
| Memorizing Transformers | `(d_key + d_value) Ã— H Ã— N Ã— S` | `N Ã— S` | ãªã— | kNN + dot-product |
| RMT | `d_model Ã— p Ã— l Ã— 2` | `N Ã— S` | ç ´æ£„ | Soft-prompt input |
| AutoCompressors | `d_model Ã— p Ã— (m + 1) Ã— l` | `N Ã— S` | ç ´æ£„ | Soft-prompt input |
| **Infini-Transformers** | `d_key Ã— (d_value + 1) Ã— H Ã— l` | `N Ã— S` | **å¢—åˆ†æ›´æ–°** | **Linear attention** |

- `N`: å…¥åŠ›ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·
- `S`: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°
- `l`: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
- `H`: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
- `c`: Compressive Transformerã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º
- `r`: åœ§ç¸®ç‡
- `p`: soft-promptã‚µãƒãƒªãƒ¼ãƒ™ã‚¯ãƒˆãƒ«æ•°
- `m`: ã‚µãƒãƒªãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—

**Infini-Transformerã®å„ªä½æ€§**: ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ä¾å­˜ã—ãªã„å®šæ•°

---

## 9. ã‚²ãƒ¼ãƒˆã‚¹ã‚³ã‚¢ã®åˆ†æï¼ˆFigure 3ï¼‰

å­¦ç¿’å¾Œã€ãƒ˜ãƒƒãƒ‰ã¯2ã‚¿ã‚¤ãƒ—ã«åˆ†åŒ–ï¼š

### 9.1 ãƒ˜ãƒƒãƒ‰ã‚¿ã‚¤ãƒ—

| ã‚¿ã‚¤ãƒ— | ã‚²ãƒ¼ãƒˆã‚¹ã‚³ã‚¢ | å½¹å‰² |
|--------|------------|------|
| **ç‰¹åŒ–å‹ï¼ˆShort-rangeï¼‰** | Î² â‰ˆ 0 | ãƒ­ãƒ¼ã‚«ãƒ«Attentionã®ã¿ä½¿ç”¨ |
| **ç‰¹åŒ–å‹ï¼ˆLong-rangeï¼‰** | Î² â‰ˆ 1 | ãƒ¡ãƒ¢ãƒªæ¤œç´¢ã®ã¿ä½¿ç”¨ |
| **æ··åˆå‹ï¼ˆMixerï¼‰** | Î² â‰ˆ 0.5 | ãƒ­ãƒ¼ã‚«ãƒ«ã¨ãƒ¡ãƒ¢ãƒªã‚’å‡ç­‰ã«çµåˆ |

### 9.2 è¦³å¯Ÿã•ã‚ŒãŸç‰¹å¾´

- **å„å±¤ã«æœ€ä½1ã¤ã®short-rangeãƒ˜ãƒƒãƒ‰ãŒå­˜åœ¨** â†’ å…¥åŠ›ä¿¡å·ãŒå‡ºåŠ›å±¤ã¾ã§ç›´æ¥ä¼æ’­å¯èƒ½
- **é•·è·é›¢ãƒ»çŸ­è·é›¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–** â†’ forwardè¨ˆç®—å…¨ä½“ã§äº¤äº’ã«ç™ºç”Ÿ

---

## 10. Multi-Head Attentionçµ±åˆ

### 10.1 ä¸¦åˆ—è¨ˆç®—

```
O = [AÂ¹; AÂ²; ... Aá´´] @ W_O
```

- å„ãƒ˜ãƒƒãƒ‰ãŒç‹¬ç«‹ã—ãŸåœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’ä¿æŒ
- Hå€‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ…‹ã‚’ä¸¦åˆ—è¨ˆç®—
- çµåˆå¾Œã«å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ `W_O âˆˆ R^(HÃ—d_valueÃ—d_model)`

### 10.2 ãƒ˜ãƒƒãƒ‰ã”ã¨ã®ç‹¬ç«‹æ€§

å„ãƒ˜ãƒƒãƒ‰ã¯ä»¥ä¸‹ã‚’ç‹¬ç«‹ã—ã¦ä¿æŒï¼š
- ãƒ¡ãƒ¢ãƒªè¡Œåˆ— `MÊ° âˆˆ R^(d_keyÃ—d_value)`
- æ­£è¦åŒ–é … `zÊ° âˆˆ R^(d_key)`
- ã‚²ãƒ¼ãƒˆã‚¹ã‚«ãƒ©ãƒ¼ `Î²Ê° âˆˆ R`

---

## 11. Passkey Retrieval Task ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆAppendix Bï¼‰

```
There is an important info hidden inside a lot of irrelevant text.
Find it and memorize them. I will quiz you about the important
information there. The grass is green. The sky is blue. The sun
is yellow. Here we go. There and back again. (repeat x times)

The pass key is 9054. Remember it. 9054 is the pass key.

The grass is green. The sky is blue. The sun is yellow. Here we go.
There and back again. (repeat y times)

What is the pass key? The pass key is
```

- `x`, `y` ã‚’èª¿æ•´ã—ã¦passkeyã®ä½ç½®ï¼ˆå…ˆé ­/ä¸­å¤®/æœ«å°¾ï¼‰ã‚’åˆ¶å¾¡
- ç¹°ã‚Šè¿”ã—ãƒ†ã‚­ã‚¹ãƒˆã§ä»»æ„ã®é•·ã•ã«æ‹¡å¼µ

---

## 12. ç†è«–çš„èƒŒæ™¯ã¨é–¢é€£ç ”ç©¶

### 12.1 é€£æƒ³ãƒ¡ãƒ¢ãƒªã®ç³»è­œ

| æ‰‹æ³• | å¹´ | ç‰¹å¾´ |
|------|-----|------|
| Hopfield Networks | 1982 | ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®é€£æƒ³è¨˜æ†¶ |
| Sparse Distributed Memory | 1988 | é«˜æ¬¡å…ƒç©ºé–“ã§ã®åˆ†æ•£è¡¨ç¾ |
| Fast Weights | 1987, 1992 | çŸ­æœŸè¨˜æ†¶ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– |
| Neural Turing Machines | 2014 | å¤–éƒ¨ãƒ¡ãƒ¢ãƒªã¸ã®èª­ã¿æ›¸ã |
| Metalearned Neural Memory (MNM) | 2019 | FFNã‚’ãƒ¡ãƒ¢ãƒªã¨ã—ã¦ä½¿ç”¨ã€QKVã§æ“ä½œ |

### 12.2 Linear Attentionã¨ã®é–¢ä¿‚

Infini-attentionã®ãƒ¡ãƒ¢ãƒªæ“ä½œã¯**Linear Attention**ï¼ˆKatharopoulos et al., 2020ï¼‰ã¨ç­‰ä¾¡ï¼š

```
# Standard attention: O(NÂ²)
A = softmax(QK^T / âˆšd) @ V

# Linear attention: O(N)
A = Ïƒ(Q) @ (Ïƒ(K)^T @ V) / (Ïƒ(Q) @ Ïƒ(K)^T @ 1)
```

é€£æƒ³è¡Œåˆ— `Ïƒ(K)^T @ V` ã¯å¤–ç©ã®ç´¯ç©ã§ã‚ã‚Šã€Infini-attentionã®ãƒ¡ãƒ¢ãƒªæ›´æ–°ã¨åŒä¸€ã€‚

### 12.3 Tensor Product Variable Bindingï¼ˆSmolensky, 1990ï¼‰

ãƒ¡ãƒ¢ãƒªæ›´æ–° `Ïƒ(K)^T @ V` ã¯**ãƒ†ãƒ³ã‚½ãƒ«ç©å¤‰æ•°ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°**ï¼š
- Key ã‚’ã€Œã‚¢ãƒ‰ãƒ¬ã‚¹ã€ã€Value ã‚’ã€Œå†…å®¹ã€ã¨ã—ã¦çµåˆ
- å¤–ç©ã«ã‚ˆã‚Šã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãªæ§‹é€ ã‚’é€£ç¶šç©ºé–“ã§è¡¨ç¾

---

## 13. æ—¢çŸ¥ã®èª²é¡Œã¨å¯¾ç­–

### 13.1 Attention Sinkå•é¡Œ

> "The attention mechanism is also prone to the issues of attention sink" (è«–æ–‡ Section 5)

**å•é¡Œ**: æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã«éå‰°ãªattentioné‡ã¿ãŒé›†ä¸­
**Infini-attentionã®å¯¾ç­–**: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå˜ä½å‡¦ç†ã§ãƒ­ãƒ¼ã‚«ãƒ«attentionã‚’å›ºå®šé•·ã«åˆ¶é™

### 13.2 Lost-in-the-Middleå•é¡Œ

> "lost-in-the-middle" (Liu et al., 2024)

**å•é¡Œ**: é•·æ–‡ã®ä¸­å¤®éƒ¨åˆ†ã®æƒ…å ±ãŒå¤±ã‚ã‚Œã‚„ã™ã„
**Infini-attentionã®å¯¾ç­–**: åœ§ç¸®ãƒ¡ãƒ¢ãƒªãŒå…¨ä½ç½®ã®æƒ…å ±ã‚’å‡ç­‰ã«ä¿æŒ

### 13.3 é•·ã•å¤–æŒ¿ã®èª²é¡Œ

> "they struggle in a regime where context length is longer than what was observed during training"

**Infini-attentionã®å®Ÿè¨¼**: 5Ké•·å­¦ç¿’ â†’ 1Mé•·è©•ä¾¡ã§æˆåŠŸ

---

## 14. ã¾ã¨ã‚

Infini-attentionã¯ï¼š
1. **å›ºå®šã‚µã‚¤ã‚ºãƒ¡ãƒ¢ãƒª**ã§ç„¡é™é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
2. **114xåœ§ç¸®ç‡**ã§Memorizing Transformersã‚’ä¸Šå›ã‚‹
3. **5Ké•·å­¦ç¿’â†’1Mé•·è©•ä¾¡**ã®å¤–æŒ¿èƒ½åŠ›ã‚’å®Ÿè¨¼
4. **Linear Attention**ã¨ãƒ†ãƒ³ã‚½ãƒ«ç©ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ç†è«–çš„åŸºç›¤
5. **Attention Sink/Lost-in-the-Middleå•é¡Œ**ã¸ã®å¯¾ç­–

ç¾åœ¨ã®Senriå®Ÿè£…ã¯åŸºæœ¬çš„ãªæ§‹é€ ã¯æ­£ã—ã„ãŒã€**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†**ã¨**æ´»æ€§åŒ–é–¢æ•°**ã®è¿½åŠ ã§è«–æ–‡ã«ã‚ˆã‚Šå¿ å®Ÿãªå®Ÿè£…ãŒå¯èƒ½ã€‚

---

## å‚è€ƒæ–‡çŒ®ï¼ˆä¸»è¦ï¼‰

- Katharopoulos et al. (2020): "Transformers are RNNs: Fast autoregressive transformers with linear attention"
- Schlag et al. (2020, 2021): "Learning associative inference using fast weight memory", "Linear transformers are secretly fast weight programmers"
- Munkhdalai et al. (2019): "Metalearned neural memory"
- Smolensky (1990): "Tensor product variable binding"
